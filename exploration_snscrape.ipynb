{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook to tinker with snscrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\AppData\\Local\\Temp\\ipykernel_15888\\384721879.py:15: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([tweet.date,tweet.id, tweet.content, tweet.user, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quotedTweet, tweet.inReplyToTweetId, tweet.inReplyToUser, tweet.mentionedUsers, tweet.hashtags])\n",
      "Stopping after 20 empty pages\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter  # Import the twitter scraping module from snscrape\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "from datetime import datetime, timedelta  # Import datetime for date manipulation\n",
    "\n",
    "# Define the end date as 6 years from now\n",
    "end_date = datetime.now() - timedelta(weeks=6 * 52)  # Calculate the date 6 years ago\n",
    "\n",
    "query = \"BLM\"  # Define the keyword for the twitter search\n",
    "number_limitation = int(1e6)  # Define the maximum number of tweets to scrape\n",
    "\n",
    "tweets = []  # Initialize a list to store the tweets\n",
    "\n",
    "# Loop through the tweets scraped by snscrape with the given query (keyword + date range)\n",
    "for i, tweet in enumerate(\n",
    "    sntwitter.TwitterSearchScraper(\n",
    "        query + \" since:\" + end_date.strftime(\"%Y-%m-%d\")\n",
    "    ).get_items()\n",
    "):\n",
    "    if (\n",
    "        i > number_limitation\n",
    "    ):  # If the number of scraped tweets exceeds the limit, stop the loop\n",
    "        break\n",
    "\n",
    "    # Append each tweet's attributes to the list. These attributes include the date, tweet ID, content, user, reply count, retweet count, like count, quoted tweet, in reply to tweet ID, in reply to user, mentioned users, and hashtags.\n",
    "    tweets.append(\n",
    "        [\n",
    "            tweet.date,\n",
    "            tweet.id,\n",
    "            tweet.content,\n",
    "            tweet.user,\n",
    "            tweet.replyCount,\n",
    "            tweet.retweetCount,\n",
    "            tweet.likeCount,\n",
    "            tweet.quotedTweet,\n",
    "            tweet.inReplyToTweetId,\n",
    "            tweet.inReplyToUser,\n",
    "            tweet.mentionedUsers,\n",
    "            tweet.hashtags,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Convert the list of tweets into a DataFrame. Each column corresponds to one of the attributes we stored in the list.\n",
    "df = pd.DataFrame(\n",
    "    tweets,\n",
    "    columns=[\n",
    "        \"date\",\n",
    "        \"id\",\n",
    "        \"content\",\n",
    "        \"user\",\n",
    "        \"replyCount\",\n",
    "        \"retweetCount\",\n",
    "        \"likeCount\",\n",
    "        \"quotedTweet\",\n",
    "        \"inReplyToTweetId\",\n",
    "        \"inReplyToUser\",\n",
    "        \"mentionedUsers\",\n",
    "        \"hashtags\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save the DataFrame to a csv file. You can specify the file path as a string. Here we're just saving it in the same directory as the script with the name 'tweets.csv'.\n",
    "df.to_csv(\"tweets.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
